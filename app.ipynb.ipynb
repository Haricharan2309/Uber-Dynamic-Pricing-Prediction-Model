{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197e9601-6f8b-45fd-8650-868e72368239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Necessary imports for data processing and modeling\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from geopy.distance import geodesic\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6819282d-61f5-4fd8-acac-8108eed69697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Data Loading and Cleaning\n",
    "file_path = 'uber.csv'  # File path for your dataset\n",
    "uber_data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert pickup_datetime to datetime and create time-related features\n",
    "uber_data['pickup_datetime'] = pd.to_datetime(uber_data['pickup_datetime'])\n",
    "uber_data['hour'] = uber_data['pickup_datetime'].dt.hour\n",
    "uber_data['day_of_week'] = uber_data['pickup_datetime'].dt.dayofweek\n",
    "uber_data['month'] = uber_data['pickup_datetime'].dt.month\n",
    "uber_data['year'] = uber_data['pickup_datetime'].dt.year\n",
    "uber_data['is_weekend'] = uber_data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Step 1: Removing Unrealistic Values\n",
    "# Remove unrealistic fare_amount values (e.g., less than zero)\n",
    "uber_data_clean = uber_data[uber_data['fare_amount'] >= 0]\n",
    "\n",
    "# Remove unrealistic passenger_count values (e.g., less than 1 or greater than 6)\n",
    "uber_data_clean = uber_data_clean[(uber_data_clean['passenger_count'] >= 1) & (uber_data_clean['passenger_count'] <= 6)]\n",
    "\n",
    "# Remove rows with invalid latitude/longitude values (filter for New York City region)\n",
    "uber_data_clean = uber_data_clean[\n",
    "    (uber_data_clean['pickup_latitude'].between(40.4774, 40.9176)) &\n",
    "    (uber_data_clean['pickup_longitude'].between(-74.2591, -73.7004)) &\n",
    "    (uber_data_clean['dropoff_latitude'].between(40.4774, 40.9176)) &\n",
    "    (uber_data_clean['dropoff_longitude'].between(-74.2591, -73.7004))\n",
    "]\n",
    "\n",
    "# Step 2: Handling Missing Values\n",
    "# Drop rows with missing values (since we only have very few missing values)\n",
    "uber_data_clean = uber_data_clean.dropna()\n",
    "\n",
    "# Step 3: Log Transformation to Reduce Skewness (for 'fare_amount' and 'distance')\n",
    "uber_data_clean['fare_amount'] = np.log1p(uber_data_clean['fare_amount'])  # log1p to handle zero values safely\n",
    "\n",
    "# Step 4: Recalculate Distance Based on Cleaned Data\n",
    "def calculate_distance(row):\n",
    "    pickup_coords = (row['pickup_latitude'], row['pickup_longitude'])\n",
    "    dropoff_coords = (row['dropoff_latitude'], row['dropoff_longitude'])\n",
    "    return geodesic(pickup_coords, dropoff_coords).miles\n",
    "\n",
    "uber_data_clean['distance'] = uber_data_clean.apply(calculate_distance, axis=1)\n",
    "\n",
    "# Step 5: One-Hot Encoding for Categorical Variables\n",
    "uber_data_clean = pd.get_dummies(uber_data_clean, columns=['day_of_week', 'hour', 'month', 'year'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732d5f97-5201-4c81-8c0a-85e6691e1158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Descriptive Statistics (Cleaned Data):\n",
      "         Unnamed: 0    fare_amount  pickup_longitude  pickup_latitude  \\\n",
      "count  1.948240e+05  194824.000000     194824.000000    194824.000000   \n",
      "mean   2.771291e+07       2.332485        -73.975513        40.750779   \n",
      "std    1.600878e+07       0.544381          0.034482         0.026933   \n",
      "min    1.000000e+00       0.000000        -74.243432        40.498988   \n",
      "25%    1.383094e+07       1.945910        -73.992277        40.736465   \n",
      "50%    2.775954e+07       2.251292        -73.982119        40.753290   \n",
      "75%    4.154486e+07       2.602690        -73.968412        40.767512   \n",
      "max    5.542357e+07       6.214608        -73.702735        40.917048   \n",
      "\n",
      "       dropoff_longitude  dropoff_latitude  passenger_count     is_weekend  \\\n",
      "count      194824.000000     194824.000000    194824.000000  194824.000000   \n",
      "mean          -73.974592         40.751041         1.689853       0.283697   \n",
      "std             0.034083          0.030687         1.305750       0.450793   \n",
      "min           -74.257692         40.486242         1.000000       0.000000   \n",
      "25%           -73.991600         40.735340         1.000000       0.000000   \n",
      "50%           -73.980558         40.753732         1.000000       0.000000   \n",
      "75%           -73.965470         40.768300         2.000000       1.000000   \n",
      "max           -73.700650         40.916956         6.000000       1.000000   \n",
      "\n",
      "            distance  \n",
      "count  194824.000000  \n",
      "mean        2.056392  \n",
      "std         2.212623  \n",
      "min         0.000000  \n",
      "25%         0.781220  \n",
      "50%         1.340686  \n",
      "75%         2.425890  \n",
      "max        21.965510  \n",
      "\n",
      "Categorical Feature Overview (Cleaned Data):\n",
      "                                key\n",
      "count                        194824\n",
      "unique                       194824\n",
      "top     2015-05-07 19:52:06.0000003\n",
      "freq                              1\n",
      "\n",
      "Missing Values in Each Column (Cleaned Data):\n",
      "Unnamed: 0           0\n",
      "key                  0\n",
      "fare_amount          0\n",
      "pickup_datetime      0\n",
      "pickup_longitude     0\n",
      "pickup_latitude      0\n",
      "dropoff_longitude    0\n",
      "dropoff_latitude     0\n",
      "passenger_count      0\n",
      "is_weekend           0\n",
      "distance             0\n",
      "day_of_week_1        0\n",
      "day_of_week_2        0\n",
      "day_of_week_3        0\n",
      "day_of_week_4        0\n",
      "day_of_week_5        0\n",
      "day_of_week_6        0\n",
      "hour_1               0\n",
      "hour_2               0\n",
      "hour_3               0\n",
      "hour_4               0\n",
      "hour_5               0\n",
      "hour_6               0\n",
      "hour_7               0\n",
      "hour_8               0\n",
      "hour_9               0\n",
      "hour_10              0\n",
      "hour_11              0\n",
      "hour_12              0\n",
      "hour_13              0\n",
      "hour_14              0\n",
      "hour_15              0\n",
      "hour_16              0\n",
      "hour_17              0\n",
      "hour_18              0\n",
      "hour_19              0\n",
      "hour_20              0\n",
      "hour_21              0\n",
      "hour_22              0\n",
      "hour_23              0\n",
      "month_2              0\n",
      "month_3              0\n",
      "month_4              0\n",
      "month_5              0\n",
      "month_6              0\n",
      "month_7              0\n",
      "month_8              0\n",
      "month_9              0\n",
      "month_10             0\n",
      "month_11             0\n",
      "month_12             0\n",
      "year_2010            0\n",
      "year_2011            0\n",
      "year_2012            0\n",
      "year_2013            0\n",
      "year_2014            0\n",
      "year_2015            0\n",
      "dtype: int64\n",
      "\n",
      "Unique Values in 'key' Column (Cleaned Data):\n",
      "key\n",
      "2015-05-07 19:52:06.0000003      1\n",
      "2014-04-11 14:50:33.0000001      1\n",
      "2012-05-11 16:59:00.000000117    1\n",
      "2015-01-23 16:55:33.0000002      1\n",
      "2010-07-20 22:01:00.00000096     1\n",
      "                                ..\n",
      "2010-05-02 18:13:46.0000002      1\n",
      "2011-08-08 10:05:00.00000094     1\n",
      "2014-03-03 07:08:00.00000023     1\n",
      "2013-10-11 07:29:25.0000001      1\n",
      "2010-05-15 04:08:00.00000076     1\n",
      "Name: count, Length: 194824, dtype: int64\n",
      "\n",
      "Correlation Matrix (Numerical Features Only, Cleaned Data):\n",
      "                   Unnamed: 0  fare_amount  pickup_longitude  pickup_latitude  \\\n",
      "Unnamed: 0           1.000000     0.000218          0.001472         0.001527   \n",
      "fare_amount          0.000218     1.000000          0.328603        -0.179881   \n",
      "pickup_longitude     0.001472     0.328603          1.000000        -0.045155   \n",
      "pickup_latitude      0.001527    -0.179881         -0.045155         1.000000   \n",
      "dropoff_longitude    0.005567     0.280903          0.300319         0.055080   \n",
      "dropoff_latitude    -0.000353    -0.161939          0.043997         0.425394   \n",
      "passenger_count      0.001693     0.016158         -0.002353        -0.009657   \n",
      "is_weekend          -0.003123     0.001795         -0.016845        -0.044243   \n",
      "distance            -0.001242     0.827673          0.435193        -0.241231   \n",
      "\n",
      "                   dropoff_longitude  dropoff_latitude  passenger_count  \\\n",
      "Unnamed: 0                  0.005567         -0.000353         0.001693   \n",
      "fare_amount                 0.280903         -0.161939         0.016158   \n",
      "pickup_longitude            0.300319          0.043997        -0.002353   \n",
      "pickup_latitude             0.055080          0.425394        -0.009657   \n",
      "dropoff_longitude           1.000000          0.134488        -0.003342   \n",
      "dropoff_latitude            0.134488          1.000000        -0.004834   \n",
      "passenger_count            -0.003342         -0.004834         1.000000   \n",
      "is_weekend                 -0.000004         -0.030440         0.042137   \n",
      "distance                    0.359720         -0.165505         0.008597   \n",
      "\n",
      "                   is_weekend  distance  \n",
      "Unnamed: 0          -0.003123 -0.001242  \n",
      "fare_amount          0.001795  0.827673  \n",
      "pickup_longitude    -0.016845  0.435193  \n",
      "pickup_latitude     -0.044243 -0.241231  \n",
      "dropoff_longitude   -0.000004  0.359720  \n",
      "dropoff_latitude    -0.030440 -0.165505  \n",
      "passenger_count      0.042137  0.008597  \n",
      "is_weekend           1.000000  0.022609  \n",
      "distance             0.022609  1.000000  \n",
      "\n",
      "Skewness and Kurtosis for 'Unnamed: 0' (Cleaned Data):\n",
      "Skewness: -0.00211625852298549\n",
      "Kurtosis: -1.1998395827446318\n",
      "\n",
      "Skewness and Kurtosis for 'fare_amount' (Cleaned Data):\n",
      "Skewness: 0.9664814064412875\n",
      "Kurtosis: 1.0011532061416366\n",
      "\n",
      "Skewness and Kurtosis for 'pickup_longitude' (Cleaned Data):\n",
      "Skewness: 3.3644551510974203\n",
      "Kurtosis: 15.17932173786701\n",
      "\n",
      "Skewness and Kurtosis for 'pickup_latitude' (Cleaned Data):\n",
      "Skewness: -0.9720262470379978\n",
      "Kurtosis: 3.440432457508148\n",
      "\n",
      "Skewness and Kurtosis for 'dropoff_longitude' (Cleaned Data):\n",
      "Skewness: 2.4125532343618\n",
      "Kurtosis: 13.840119753347782\n",
      "\n",
      "Skewness and Kurtosis for 'dropoff_latitude' (Cleaned Data):\n",
      "Skewness: -0.5605986504265458\n",
      "Kurtosis: 3.3760984475859974\n",
      "\n",
      "Skewness and Kurtosis for 'passenger_count' (Cleaned Data):\n",
      "Skewness: 1.9810929822102437\n",
      "Kurtosis: 2.7626275653995185\n",
      "\n",
      "Skewness and Kurtosis for 'is_weekend' (Cleaned Data):\n",
      "Skewness: 0.9596582101826532\n",
      "Kurtosis: -1.0790561196290265\n",
      "\n",
      "Skewness and Kurtosis for 'distance' (Cleaned Data):\n",
      "Skewness: 2.8761496899659784\n",
      "Kurtosis: 10.131480235721085\n"
     ]
    }
   ],
   "source": [
    "# Block 2.1: Descriptive Statistics After Data Cleaning\n",
    "# Descriptive statistics summary for numerical features\n",
    "print(\"Numerical Descriptive Statistics (Cleaned Data):\")\n",
    "print(uber_data_clean.describe())\n",
    "\n",
    "# Descriptive statistics summary for categorical features (if any)\n",
    "print(\"\\nCategorical Feature Overview (Cleaned Data):\")\n",
    "print(uber_data_clean.describe(include=['O']))  # 'O' represents object data type, typically used for categorical data\n",
    "\n",
    "# Count missing values for each column\n",
    "print(\"\\nMissing Values in Each Column (Cleaned Data):\")\n",
    "print(uber_data_clean.isna().sum())\n",
    "\n",
    "# Count of unique values for each categorical feature\n",
    "categorical_columns = uber_data_clean.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    print(f\"\\nUnique Values in '{column}' Column (Cleaned Data):\")\n",
    "    print(uber_data_clean[column].value_counts())\n",
    "\n",
    "# Correlation matrix to understand relationships between numerical features only\n",
    "print(\"\\nCorrelation Matrix (Numerical Features Only, Cleaned Data):\")\n",
    "numerical_data = uber_data_clean.select_dtypes(include=['float64', 'int64'])\n",
    "print(numerical_data.corr())\n",
    "\n",
    "# Additional custom summary for specific insights (e.g., range, skewness, etc.)\n",
    "# Calculating skewness and kurtosis for numerical columns\n",
    "for column in numerical_data.columns:\n",
    "    column_skewness = skew(numerical_data[column].dropna())\n",
    "    column_kurtosis = kurtosis(numerical_data[column].dropna())\n",
    "    print(f\"\\nSkewness and Kurtosis for '{column}' (Cleaned Data):\")\n",
    "    print(f\"Skewness: {column_skewness}\")\n",
    "    print(f\"Kurtosis: {column_kurtosis}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c8475f-5c85-44c9-9f89-d4664f555631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Skewness for 'distance':\n",
      "0.9662331991613881\n"
     ]
    }
   ],
   "source": [
    "# Block 2.2: Additional Data Cleaning\n",
    "# Drop the 'key' column as it is not useful for modeling\n",
    "uber_data_clean = uber_data_clean.drop(columns=['key', 'Unnamed: 0'])\n",
    "\n",
    "# Optional: Apply log transformation to highly skewed features\n",
    "uber_data_clean['distance'] = np.log1p(uber_data_clean['distance'])  # Log transform distance to reduce skewness\n",
    "\n",
    "# Check skewness again after transformation\n",
    "print(\"\\nUpdated Skewness for 'distance':\")\n",
    "print(skew(uber_data_clean['distance'].dropna()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1fafbc-23fb-412b-bdda-0d7546ecb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Train-Test Split\n",
    "X = uber_data_clean.drop(columns=['fare_amount', 'pickup_datetime'])\n",
    "y = uber_data_clean['fare_amount']\n",
    "\n",
    "# Train-test split (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddabb8-11c6-45c5-91af-97676fa33b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Hyperparameter Tuning with RandomizedSearchCV\n",
    "# Define hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': [200, 300, 400, 500],  # Increase number of boosting rounds\n",
    "    'max_depth': [4, 6, 8, 10],  # Deeper trees for more complex relationships\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Try a range of learning rates\n",
    "    'subsample': [0.6, 0.8, 1.0],  # Use different subsets of data per boosting round\n",
    "    'reg_alpha': [0, 0.01, 0.1],  # L1 regularization (Lasso)\n",
    "    'reg_lambda': [1, 5, 10]  # L2 regularization (Ridge)\n",
    "}\n",
    "\n",
    "# Initialize XGBoost Regressor\n",
    "xgb_regressor = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Randomized Search with 5-fold Cross-Validation\n",
    "random_search = RandomizedSearchCV(estimator=xgb_regressor, param_distributions=param_dist, n_iter=20, \n",
    "                                   scoring='neg_mean_squared_error', n_jobs=-1, cv=5, verbose=2, random_state=42)\n",
    "\n",
    "# Fit Randomized Search to the Training Data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b56bd-4f8a-4d56-85e8-c05399c2b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Training the Best Model with Early Stopping using DMatrix\n",
    "# Convert the data into DMatrix format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define the parameters using the best params from RandomizedSearch\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': best_params['learning_rate'],\n",
    "    'max_depth': best_params['max_depth'],\n",
    "    'subsample': best_params['subsample'],\n",
    "    'reg_alpha': best_params['reg_alpha'],\n",
    "    'reg_lambda': best_params['reg_lambda']\n",
    "}\n",
    "\n",
    "# Training with early stopping\n",
    "evals = [(dtrain, 'train'), (dtest, 'eval')]  # Evaluation sets\n",
    "bst_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=best_params['n_estimators'],\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=15,  # Stop after 15 rounds without improvement\n",
    "    verbose_eval=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748a488-dcf5-45b6-8e63-40d6b2b6c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Cross-Validation Evaluation\n",
    "# Cross-validation to evaluate the model\n",
    "cv_scores = cross_val_score(random_search.best_estimator_, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse_scores = np.sqrt(-cv_scores)\n",
    "print(f\"Cross-Validation RMSE Scores: {cv_rmse_scores}\")\n",
    "print(f\"Average Cross-Validation RMSE: {np.mean(cv_rmse_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd1676-7a2b-413f-990f-35123038d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Model Evaluation on the Test Set\n",
    "# Make predictions on the test set\n",
    "y_pred = bst_model.predict(dtest)\n",
    "\n",
    "# Evaluate performance\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)  # Use squared=False for RMSE\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared value\n",
    "\n",
    "# Output the performance\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R-squared (RÂ²): {r2}\")\n",
    "\n",
    "# Final Performance\n",
    "print(f\"Final Root Mean Squared Error: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee0e89-9e54-4fcc-b1fb-50c977513479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
